{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from spacy.symbols import ORTH\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>text</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>58522792</td>\n",
       "      <td>16567081</td>\n",
       "      <td>b\"                                 FINAL REPOR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>58213163</td>\n",
       "      <td>16567081</td>\n",
       "      <td>b'                                 FINAL REPOR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>59835582</td>\n",
       "      <td>16043746</td>\n",
       "      <td>b'                                 FINAL REPOR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>51487790</td>\n",
       "      <td>16456872</td>\n",
       "      <td>b'                                 FINAL REPOR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>59750073</td>\n",
       "      <td>16824069</td>\n",
       "      <td>b'                                 FINAL REPOR...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id  subject_id                                               text  \\\n",
       "0  58522792    16567081  b\"                                 FINAL REPOR...   \n",
       "1  58213163    16567081  b'                                 FINAL REPOR...   \n",
       "2  59835582    16043746  b'                                 FINAL REPOR...   \n",
       "3  51487790    16456872  b'                                 FINAL REPOR...   \n",
       "4  59750073    16824069  b'                                 FINAL REPOR...   \n",
       "\n",
       "   Atelectasis  Cardiomegaly  Consolidation  Edema  \\\n",
       "0          0.0           0.0            0.0    0.0   \n",
       "1          0.0           1.0            0.0    1.0   \n",
       "2          0.0           0.0            0.0    0.0   \n",
       "3          0.0           0.0            0.0    0.0   \n",
       "4          0.0           0.0            0.0    0.0   \n",
       "\n",
       "   Enlarged Cardiomediastinum  Fracture  Lung Lesion  Lung Opacity  \\\n",
       "0                         0.0       0.0          0.0           0.0   \n",
       "1                         0.0       0.0          0.0           0.0   \n",
       "2                         0.0       0.0          0.0           0.0   \n",
       "3                         0.0       0.0          0.0           0.0   \n",
       "4                         0.0       0.0          0.0           0.0   \n",
       "\n",
       "   No Finding  Pleural Effusion  Pleural Other  Pneumonia  Pneumothorax  \\\n",
       "0         0.0               0.0            0.0        1.0           0.0   \n",
       "1         0.0               1.0            0.0        0.0           0.0   \n",
       "2         1.0               0.0            0.0        0.0           0.0   \n",
       "3         1.0               0.0            0.0        0.0           0.0   \n",
       "4         1.0               0.0            0.0        0.0           0.0   \n",
       "\n",
       "   Support Devices  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_data = pd.read_csv(\"data/text_binary.csv\")\n",
    "mimic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>No Finding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b\"                                 final repor...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  No Finding\n",
       "0  b\"                                 final repor...         0.0\n",
       "1  b'                                 final repor...         0.0\n",
       "2  b'                                 final repor...         1.0\n",
       "3  b'                                 final repor...         1.0\n",
       "4  b'                                 final repor...         1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_data = mimic_data[['text','No Finding']]\n",
    "binary_data.text = binary_data.text.str.lower()\n",
    "binary_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Finding</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>152372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>75455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text\n",
       "No Finding        \n",
       "0.0         152372\n",
       "1.0          75455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_data.groupby('No Finding').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"                                 final report\\\\n type of examination:  chest pa and lateral.\\\\n \\\\n indication:  ___-year-old male patient with recent pneumonia diagnosed and\\\\n treated at another facility.  x-ray not available, now with continued cough\\\\n and wheeze, history of copd, remaining evidence of pneumonia?\\\\n \\\\n findings:  pa and lateral chest views were obtained with patient in upright\\\\n position.  analysis is performed in direct comparison with the next preceding\\\\n chest examination of ___.  the heart size remains normal.  no\\\\n typical configurational abnormality is seen.  the thoracic aorta is moderately\\\\n widened and somewhat elongated but no local contour abnormalities are\\\\n identified.  the pulmonary vasculature is not congested.  there exists,\\\\n however, some irregular peripheral vascular distribution most marked on the\\\\n bases and coinciding with some slightly hypertranslucent pulmonary areas and\\\\n flattened low positioned diaphragms are indicative of copd.  when direct\\\\n comparison is made with the previous examination of ___, there is a hazy mild\\\\n degree of density in the left base shows a corresponding local thickening of\\\\n the lower portion of the major fissure on the left side.  this finding is\\\\n consistent with some resolving pneumonic process such as recent pneumonia in\\\\n resolution.  skeletal changes which were characterized by some longitudinal\\\\n calcification in the anterior ligaments of the thoracic spine appears stable\\\\n and has not progressed.  no new skeletal abnormalities identified.\\\\n \\\\n impression:  findings of general copd as before, minor pleural and parenchymal\\\\n hazy densities on the left base probably pneumonia in resolution and match the\\\\n clinical report of a recent pneumonia.  if patient\\'s symptoms continue,\\\\n recommend another follow up examination in a week or so.\\\\n\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_data.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = binary_data['No Finding']\n",
    "X = binary_data.drop(columns=['No Finding'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b\" wet read: ___ ___ 9:56 pm\\n  new left pleur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  b'                                 final repor...\n",
       "1  b\" wet read: ___ ___ 9:56 pm\\n  new left pleur...\n",
       "2  b'                                 final repor...\n",
       "3  b'                                 final repor...\n",
       "4  b'                                 final repor..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'                                 final report\\\\n indication:  ___-year-old female with cough and fever.  please perform a\\\\n lateral view to evaluate for pneumonia.\\\\n \\\\n comparison:  frontal chest radiograph performed approximately one hour prior\\\\n to this exam.\\\\n \\\\n technique:  a leftward rotated ap view and a lateral view of the chest were\\\\n obtained.\\\\n \\\\n findings:  the frontal view is extremely rotated to the left, with complete\\\\n projection of the mediastinum over the left lung, which limits assessment. \\\\n the expanded right lung is unremarkable.  assessment in the lateral view is\\\\n also limited due to superimposition of the arms, but allowing for technical\\\\n limitations, there is no spine sign, pleural effusion, or abnormality in the\\\\n anterior mediastinum. no pneumothorax is identified.  artifacts from external\\\\n hair devices are again seen.\\\\n \\\\n impression:  limited examination.  no evidence of acute cardiopulmonary\\\\n process.\\\\n'\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all '\\\\n' from the text\n",
    "re_newlines = re.compile('\\\\\\\\n')\n",
    "def sub_newlines(x): return re_newlines.sub('',x)\n",
    "\n",
    "# remove all special characters from the text, keep only alphanumeric and spaces\n",
    "re_letters = re.compile('[^A-Za-z0-9 ]')\n",
    "def sub_letters(x): return re_letters.sub('', x)\n",
    "\n",
    "# remove excessive spacing otherwise you end up with \" \" substrings\n",
    "re_spaces = re.compile('\\s+')\n",
    "def sub_spaces(x): return re_spaces.sub(' ', x)\n",
    "                \n",
    "# tokenize all words.\n",
    "my_tok = spacy.load('en')\n",
    "def spacy_tok(x): \n",
    "    return [tok.text for tok in my_tok.tokenizer(sub_spaces\n",
    "                                                 (sub_letters\n",
    "                                                 (sub_newlines(x))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b final report type of examination chest pa and lateral indication yearold male patient with recent pneumonia diagnosed and treated at another facility xray not available now with continued cough and wheeze history of copd remaining evidence of pneumonia findings pa and lateral chest views were obtained with patient in upright position analysis is performed in direct comparison with the next preceding chest examination of the heart size remains normal no typical configurational abnormality is seen the thoracic aorta is moderately widened and somewhat elongated but no local contour abnormalities are identified the pulmonary vasculature is not congested there exists however some irregular peripheral vascular distribution most marked on the bases and coinciding with some slightly hypertranslucent pulmonary areas and flattened low positioned diaphragms are indicative of copd when direct comparison is made with the previous examination of there is a hazy mild degree of density in the left base shows a corresponding local thickening of the lower portion of the major fissure on the left side this finding is consistent with some resolving pneumonic process such as recent pneumonia in resolution skeletal changes which were characterized by some longitudinal calcification in the anterior ligaments of the thoracic spine appears stable and has not progressed no new skeletal abnormalities identified impression findings of general copd as before minor pleural and parenchymal hazy densities on the left base probably pneumonia in resolution and match the clinical report of a recent pneumonia if patients symptoms continue recommend another follow up examination in a week or so'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_spaces(sub_letters(sub_newlines(binary_data.text[0].lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b',\n",
       " 'final',\n",
       " 'report',\n",
       " 'type',\n",
       " 'of',\n",
       " 'examination',\n",
       " 'chest',\n",
       " 'pa',\n",
       " 'and',\n",
       " 'lateral',\n",
       " 'indication',\n",
       " 'yearold',\n",
       " 'male',\n",
       " 'patient',\n",
       " 'with',\n",
       " 'recent',\n",
       " 'pneumonia',\n",
       " 'diagnosed',\n",
       " 'and',\n",
       " 'treated',\n",
       " 'at',\n",
       " 'another',\n",
       " 'facility',\n",
       " 'xray',\n",
       " 'not',\n",
       " 'available',\n",
       " 'now',\n",
       " 'with',\n",
       " 'continued',\n",
       " 'cough',\n",
       " 'and',\n",
       " 'wheeze',\n",
       " 'history',\n",
       " 'of',\n",
       " 'copd',\n",
       " 'remaining',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'pneumonia',\n",
       " 'findings',\n",
       " 'pa',\n",
       " 'and',\n",
       " 'lateral',\n",
       " 'chest',\n",
       " 'views',\n",
       " 'were',\n",
       " 'obtained',\n",
       " 'with',\n",
       " 'patient',\n",
       " 'in',\n",
       " 'upright',\n",
       " 'position',\n",
       " 'analysis',\n",
       " 'is',\n",
       " 'performed',\n",
       " 'in',\n",
       " 'direct',\n",
       " 'comparison',\n",
       " 'with',\n",
       " 'the',\n",
       " 'next',\n",
       " 'preceding',\n",
       " 'chest',\n",
       " 'examination',\n",
       " 'of',\n",
       " 'the',\n",
       " 'heart',\n",
       " 'size',\n",
       " 'remains',\n",
       " 'normal',\n",
       " 'no',\n",
       " 'typical',\n",
       " 'configurational',\n",
       " 'abnormality',\n",
       " 'is',\n",
       " 'seen',\n",
       " 'the',\n",
       " 'thoracic',\n",
       " 'aorta',\n",
       " 'is',\n",
       " 'moderately',\n",
       " 'widened',\n",
       " 'and',\n",
       " 'somewhat',\n",
       " 'elongated',\n",
       " 'but',\n",
       " 'no',\n",
       " 'local',\n",
       " 'contour',\n",
       " 'abnormalities',\n",
       " 'are',\n",
       " 'identified',\n",
       " 'the',\n",
       " 'pulmonary',\n",
       " 'vasculature',\n",
       " 'is',\n",
       " 'not',\n",
       " 'congested',\n",
       " 'there',\n",
       " 'exists',\n",
       " 'however',\n",
       " 'some',\n",
       " 'irregular',\n",
       " 'peripheral',\n",
       " 'vascular',\n",
       " 'distribution',\n",
       " 'most',\n",
       " 'marked',\n",
       " 'on',\n",
       " 'the',\n",
       " 'bases',\n",
       " 'and',\n",
       " 'coinciding',\n",
       " 'with',\n",
       " 'some',\n",
       " 'slightly',\n",
       " 'hypertranslucent',\n",
       " 'pulmonary',\n",
       " 'areas',\n",
       " 'and',\n",
       " 'flattened',\n",
       " 'low',\n",
       " 'positioned',\n",
       " 'diaphragms',\n",
       " 'are',\n",
       " 'indicative',\n",
       " 'of',\n",
       " 'copd',\n",
       " 'when',\n",
       " 'direct',\n",
       " 'comparison',\n",
       " 'is',\n",
       " 'made',\n",
       " 'with',\n",
       " 'the',\n",
       " 'previous',\n",
       " 'examination',\n",
       " 'of',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'hazy',\n",
       " 'mild',\n",
       " 'degree',\n",
       " 'of',\n",
       " 'density',\n",
       " 'in',\n",
       " 'the',\n",
       " 'left',\n",
       " 'base',\n",
       " 'shows',\n",
       " 'a',\n",
       " 'corresponding',\n",
       " 'local',\n",
       " 'thickening',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lower',\n",
       " 'portion',\n",
       " 'of',\n",
       " 'the',\n",
       " 'major',\n",
       " 'fissure',\n",
       " 'on',\n",
       " 'the',\n",
       " 'left',\n",
       " 'side',\n",
       " 'this',\n",
       " 'finding',\n",
       " 'is',\n",
       " 'consistent',\n",
       " 'with',\n",
       " 'some',\n",
       " 'resolving',\n",
       " 'pneumonic',\n",
       " 'process',\n",
       " 'such',\n",
       " 'as',\n",
       " 'recent',\n",
       " 'pneumonia',\n",
       " 'in',\n",
       " 'resolution',\n",
       " 'skeletal',\n",
       " 'changes',\n",
       " 'which',\n",
       " 'were',\n",
       " 'characterized',\n",
       " 'by',\n",
       " 'some',\n",
       " 'longitudinal',\n",
       " 'calcification',\n",
       " 'in',\n",
       " 'the',\n",
       " 'anterior',\n",
       " 'ligaments',\n",
       " 'of',\n",
       " 'the',\n",
       " 'thoracic',\n",
       " 'spine',\n",
       " 'appears',\n",
       " 'stable',\n",
       " 'and',\n",
       " 'has',\n",
       " 'not',\n",
       " 'progressed',\n",
       " 'no',\n",
       " 'new',\n",
       " 'skeletal',\n",
       " 'abnormalities',\n",
       " 'identified',\n",
       " 'impression',\n",
       " 'findings',\n",
       " 'of',\n",
       " 'general',\n",
       " 'copd',\n",
       " 'as',\n",
       " 'before',\n",
       " 'minor',\n",
       " 'pleural',\n",
       " 'and',\n",
       " 'parenchymal',\n",
       " 'hazy',\n",
       " 'densities',\n",
       " 'on',\n",
       " 'the',\n",
       " 'left',\n",
       " 'base',\n",
       " 'probably',\n",
       " 'pneumonia',\n",
       " 'in',\n",
       " 'resolution',\n",
       " 'and',\n",
       " 'match',\n",
       " 'the',\n",
       " 'clinical',\n",
       " 'report',\n",
       " 'of',\n",
       " 'a',\n",
       " 'recent',\n",
       " 'pneumonia',\n",
       " 'if',\n",
       " 'patients',\n",
       " 'symptoms',\n",
       " 'continue',\n",
       " 'recommend',\n",
       " 'another',\n",
       " 'follow',\n",
       " 'up',\n",
       " 'examination',\n",
       " 'in',\n",
       " 'a',\n",
       " 'week',\n",
       " 'or',\n",
       " 'so']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tok(binary_data.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>No Finding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b\"                                 final repor...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b'                                 final repor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  No Finding\n",
       "0  b\"                                 final repor...         0.0\n",
       "1  b'                                 final repor...         0.0\n",
       "2  b'                                 final repor...         1.0\n",
       "3  b'                                 final repor...         1.0\n",
       "4  b'                                 final repor...         1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_train = binary_data.copy()\n",
    "binary_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(text):\n",
    "    counts = Counter()\n",
    "    for word in text:\n",
    "        counts.update(spacy_tok(word))\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = get_counts(binary_train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37093"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in list(word_count):\n",
    "    if word_count[word] < 3:\n",
    "        del word_count[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15527"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2index = {\"<PAD>\":0, \"UNK\":1} # init with padding and unknown\n",
    "words = [\"<PAD>\", \"UNK\"]\n",
    "for word in word_count:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15529"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text):\n",
    "    enc = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in text.split()])\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   3,   1,   5,   6,   1,   8,   9,  10,   1,   1,   1,   1,\n",
       "        14,  15,  16,  17,  18,  19,   1,  20,  21,  22,   1,   1,  25,\n",
       "         1,  27,  16,  28,   1,  10,   1,  31,   6,   1,  33,  34,   6,\n",
       "         1,   1,   1,   9,  10,  11,   8,  36,  37,  38,  16,  15,  39,\n",
       "         1,   1,  42,  43,  44,  39,  45,  46,  16,  47,  48,   1,   8,\n",
       "         7,   6,   1,  47,  50,  51,  52,   1,   1,  55,  56,  57,  43,\n",
       "         1,  47,  59,  60,  43,   1,  62,  10,  63,  64,  65,  54,  66,\n",
       "        67,  68,   1,   1,  47,  71,  72,  43,  25,   1,  74,   1,   1,\n",
       "        77,  78,  79,  80,  81,  82,  83,  84,   1,  85,  10,  86,  16,\n",
       "        77,  87,  88,  71,  89,   1,  90,  91,  92,  93,  69,  94,   6,\n",
       "         1,  95,   1,  46,  43,  96,  16,  47,  97,   7,   6,   1,  74,\n",
       "        43,  98,  99,   1, 101,   6, 102,  39,  47, 103, 104, 105,  98,\n",
       "       106,  66, 107,   1,  47, 108, 109,   6,  47, 110, 111,  84,  47,\n",
       "       103,   1, 113, 114,   1, 115,  16,  77, 116, 117, 118, 119, 120,\n",
       "        17,  18,   1,   1, 122, 123, 124,  37, 125, 126,  77,   1, 128,\n",
       "        39,  47, 129,   1,   6,  47,  59, 130, 131,   1,  10, 133,  25,\n",
       "         1,  54, 135, 122,  68,   1,   1,   1,  35,   6, 137,  32, 120,\n",
       "         1, 139, 140,  10,   1,  99, 142,  84,  47, 103, 104, 143,  18,\n",
       "        39, 121,  10, 144,   1, 145,   4,   6,  98,  17,   1, 146,   1,\n",
       "       148,   1, 150,  22, 151, 152,   7,  39,  98, 153, 154,   1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence(binary_train.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_Mimic(Dataset):\n",
    "    def __init__(self, X, y, vocab):\n",
    "        self.x = [encode_sentence(x) for x in X.text]\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train = Binary_Mimic(X_train, y_train, vocab2index)\n",
    "binary_valid = Binary_Mimic(X_val, y_val, vocab2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (sentences, labels).\n",
    "    \n",
    "    Need custom collate_fn because merging sequences (including padding) is not \n",
    "    supported in default. Sequences are padded to the maximum length of mini-batch \n",
    "    sequences (dynamic padding).\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuple (sentence, label). \n",
    "            - list of word indices of variable length\n",
    "            - label, 0 or 1\n",
    "    Returns:\n",
    "        packed_batch: (PackedSequence), see torch.nn.utils.rnn.pack_padded_sequence\n",
    "        sencences: torch tensor of shape (batch_size, max_len).\n",
    "        labels: torch tensor of shape (batch_size, 1).\n",
    "        lengths: list; valid length for each padded sentence. \n",
    "    \"\"\"\n",
    "    # Sort a data list by sentences length (descending order).\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    sentences, labels = zip(*data)\n",
    "    \n",
    "    # stack labels\n",
    "    labels = torch.Tensor(labels)\n",
    "    \n",
    "    # Merge sentences\n",
    "    lengths = [len(s) for s in sentences]\n",
    "   \n",
    "    sents = torch.zeros(len(sentences), max(lengths)).long()\n",
    "    for i, s in enumerate(sentences):\n",
    "        end = lengths[i]\n",
    "        sents[i, :end] = torch.Tensor(s[:end])        \n",
    "    \n",
    "    return sents, lengths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([   1,    3,    1,    1,    1,  396,   16,   29,   10,    1,  528,\n",
       "         4214,    1,   11,  475,  166,  190,  191,    1,    1,    1,  174,\n",
       "            8,  175,   44, 1031,  427, 1776,    1,  166,  113,    1,    1,\n",
       "            1,   98, 1616,  739,  194,  475,   10,   98,   11,  475,    6,\n",
       "           47,    8,    1,    1,    1,    1,   47,  174,  475,   43, 2208,\n",
       "          739,  166,   47,    1,   16,    1, 1310,    6,   47,  677,  411,\n",
       "           47,  103,    1,  124,  761,    1,    1,   47,  493,  169,  239,\n",
       "           43,    1, 1067,   39,   47,   11,  475,    1,  675,  694,  283,\n",
       "          166, 2084,    6,   47,    1,   65,  330,  191,    1,    1,   74,\n",
       "           43,   54,  130,    1,  140,    1,  154,   57,   39,    1,  129,\n",
       "            1,   54,  204,   43,    1, 4226,  183,    1, 5668,  296,   69,\n",
       "          355,    1,    1,    1,  694,    1,   54,   34,    6,  192,    1,\n",
       "            1]), 1.0),\n",
       " (array([   1,  284,    1,    1,    1,    1,    1,  135,  103,  140,    1,\n",
       "          483,  214,    1,  254,    1,  810,  102,    1,  294,  328,    1,\n",
       "          490,   16,    1,    1,  126,  717,   21,    1,   84,    1,   21,\n",
       "          113,    1,    8,  828,  448,    1,    1,    3,    1,  232,  194,\n",
       "           40,    8,  353,    1,   21,    1,    1,    1,  145,    1,    1,\n",
       "           16,  281,    1,  591,    1,    1,   46,   43,   96,  166,   47,\n",
       "            1,  302,  303,    6,    1,   21,    1,    1,  513,  232,  194,\n",
       "           40,    8,  353,    1,   21,    1,   43,    1,    1,    1,    1,\n",
       "            1,  297,   91,  239,  373,   16,  630,  388,   21,   47,  103,\n",
       "          104,  124,    1,  444,    1,  680,  495,  154,   18,    1,  254,\n",
       "            1,    1,    1,   95,  251,  166,   47,  302,  303,   74,   43,\n",
       "           54,    1,  361,    1,   54,   34,    6,   71,    1,   54,    1,\n",
       "            1,  273,   10,  198,  200,   69,  132,  421,   91,  239,  373,\n",
       "            1,  331,   39,   15,  889,  332,    1,  568, 1676,    6,    1,\n",
       "          291,   47,    1,  103,  222,   52,    1,   54,    1,    1]), 0.0),\n",
       " (array([   1,    3,    1,    1,    8,    1,    1,    1,    1,    1,  157,\n",
       "          158,  159,   16,    8,  359,   39,    1,    1,  642,  191,    1,\n",
       "            1,  320,   39,  140,    1,    1,    1,    8,  513,    1,    1,\n",
       "            1,    1, 2530,    1,    1,    1,    1,    1,  279,  169,  140,\n",
       "         2535,   39,    1,  132,  169,  140,    1,    1,  669,  169,  239,\n",
       "            1,  103,  239,   43,    1,    1,    1,    1,    1,  274,  669,\n",
       "          280,  169,    1,  132,  169,  140,    1]), 0.0)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [binary_train[0], binary_train[1], binary_train[2]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   1,  284,    1,    1,    1,    1,    1,  135,  103,  140,    1,  483,\n",
       "           214,    1,  254,    1,  810,  102,    1,  294,  328,    1,  490,   16,\n",
       "             1,    1,  126,  717,   21,    1,   84,    1,   21,  113,    1,    8,\n",
       "           828,  448,    1,    1,    3,    1,  232,  194,   40,    8,  353,    1,\n",
       "            21,    1,    1,    1,  145,    1,    1,   16,  281,    1,  591,    1,\n",
       "             1,   46,   43,   96,  166,   47,    1,  302,  303,    6,    1,   21,\n",
       "             1,    1,  513,  232,  194,   40,    8,  353,    1,   21,    1,   43,\n",
       "             1,    1,    1,    1,    1,  297,   91,  239,  373,   16,  630,  388,\n",
       "            21,   47,  103,  104,  124,    1,  444,    1,  680,  495,  154,   18,\n",
       "             1,  254,    1,    1,    1,   95,  251,  166,   47,  302,  303,   74,\n",
       "            43,   54,    1,  361,    1,   54,   34,    6,   71,    1,   54,    1,\n",
       "             1,  273,   10,  198,  200,   69,  132,  421,   91,  239,  373,    1,\n",
       "           331,   39,   15,  889,  332,    1,  568, 1676,    6,    1,  291,   47,\n",
       "             1,  103,  222,   52,    1,   54,    1,    1],\n",
       "         [   1,    3,    1,    1,    1,  396,   16,   29,   10,    1,  528, 4214,\n",
       "             1,   11,  475,  166,  190,  191,    1,    1,    1,  174,    8,  175,\n",
       "            44, 1031,  427, 1776,    1,  166,  113,    1,    1,    1,   98, 1616,\n",
       "           739,  194,  475,   10,   98,   11,  475,    6,   47,    8,    1,    1,\n",
       "             1,    1,   47,  174,  475,   43, 2208,  739,  166,   47,    1,   16,\n",
       "             1, 1310,    6,   47,  677,  411,   47,  103,    1,  124,  761,    1,\n",
       "             1,   47,  493,  169,  239,   43,    1, 1067,   39,   47,   11,  475,\n",
       "             1,  675,  694,  283,  166, 2084,    6,   47,    1,   65,  330,  191,\n",
       "             1,    1,   74,   43,   54,  130,    1,  140,    1,  154,   57,   39,\n",
       "             1,  129,    1,   54,  204,   43,    1, 4226,  183,    1, 5668,  296,\n",
       "            69,  355,    1,    1,    1,  694,    1,   54,   34,    6,  192,    1,\n",
       "             1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [   1,    3,    1,    1,    8,    1,    1,    1,    1,    1,  157,  158,\n",
       "           159,   16,    8,  359,   39,    1,    1,  642,  191,    1,    1,  320,\n",
       "            39,  140,    1,    1,    1,    8,  513,    1,    1,    1,    1, 2530,\n",
       "             1,    1,    1,    1,    1,  279,  169,  140, 2535,   39,    1,  132,\n",
       "           169,  140,    1,    1,  669,  169,  239,    1,  103,  239,   43,    1,\n",
       "             1,    1,    1,    1,  274,  669,  280,  169,    1,  132,  169,  140,\n",
       "             1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " [164, 133, 73],\n",
       " tensor([0., 1., 0.]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_fn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    3,    1,  232,    8,    1,   16,   46,    1,    1,    1,    1,\n",
       "          361, 1038,   10, 1084,    6,  452,    1,    1,  434,    6,  169,  229,\n",
       "          198,    1,  377,  283,  166,   98,  811,    1,   80, 2420,   10,  240,\n",
       "          198,  681,   84,   17,  245,    1,    1,  132,  180,  666,  126,   71,\n",
       "            1,  515,   10,  100,    1,  297,  299,  140,  271,    1,  438,  581,\n",
       "          239,  280,  124,  443,  444,  551,    1,  393,    1,  321, 2676,    6,\n",
       "         1078,  109,    6,  229,    1,  684,   47,  932,    6,    1],\n",
       "        [   1,    3,    1,    1,    8,    1,   10,    1,    1,    1,    1,  157,\n",
       "          158,  159,    1, 1678,    1,  642,  164,  642,    1,    1,    1,    8,\n",
       "          195,    1,  536,    1,    1,    1,    1,    1,  163,  299, 1209, 1550,\n",
       "          610,  861,  258,    1,    1,  550,  551,  133,    1,  163,  299,  140,\n",
       "          271,    1,    1,  225,  226,   43,   98,   53, 1210,    1,   74,   43,\n",
       "           54,   71,    1,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(binary_train, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "sents, lengths, labels = next(iter(train_loader))\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 82]), torch.Size([2]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[82, 64]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(model, optimizer, train_dl, valid_dl, epochs=10):\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, s, y in train_dl:\n",
    "            x = x.long()#.cuda()\n",
    "            y = y.float()#.cuda()\n",
    "            y_pred = model(x, s)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc = val_metrics(model, valid_dl)\n",
    "        print(\"Epoch #%.f: train loss %.3f val loss %.3f and val accuracy %.3f\" % \n",
    "              (i+1,sum_loss/total, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x, s, y in valid_dl:\n",
    "        x = x.long()#.cuda()\n",
    "        y = y.float().unsqueeze(1)#.cuda()\n",
    "        y_hat = model(x, s)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_pred = y_hat > 0\n",
    "        correct += (y_pred.float() == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super(GRUModel,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        print(x.shape)\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        pack = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        out_pack, ht = self.gru(pack)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50000\n",
    "train_dl = DataLoader(binary_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(binary_valid, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15529\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "model = GRUModel(vocab_size, 50, 50)#.cuda()\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 477])\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, optimizer, train_dl, valid_dl, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Glove to create pre-trained embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile='data/glove.6B.50d.txt'):\n",
    "    \"\"\" Loads word vectors into a dictionary.\"\"\"\n",
    "    f = open(gloveFile,'r')\n",
    "    word_vecs = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        word_vecs[word] = np.array([float(val) for val in splitLine[1:]])\n",
    "    return word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = loadGloveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = get_counts(binary_train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 37093\n"
     ]
    }
   ],
   "source": [
    "print(len(word_vecs.keys()), len(word_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rare_words(word_vecs, word_count, min_df=4):\n",
    "    \"\"\" Deletes rare words from word_count\n",
    "    \n",
    "    Deletes words from word_count if they are not in word_vecs\n",
    "    and don't have at least min_df occurrencies in word_count.\n",
    "    \"\"\"\n",
    "    words_delete = []\n",
    "    for word in word_count:\n",
    "        if word_count[word] < min_df and word not in word_vecs:\n",
    "            words_delete.append(word)\n",
    "    for word in words_delete: word_count.pop(word)\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20222\n"
     ]
    }
   ],
   "source": [
    "word_count = delete_rare_words(word_vecs, word_count, min_df=3)\n",
    "print(len(word_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_vecs, word_count, min_df=4, emb_size=50):\n",
    "    \"\"\"Creates embedding matrix from word vectors. \"\"\"\n",
    "    word_count = delete_rare_words(word_vecs, word_count, min_df)\n",
    "    V = len(word_count.keys()) + 2\n",
    "    vocab2index = {}\n",
    "    W = np.zeros((V, emb_size), dtype=\"float32\")\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    # adding a vector for padding\n",
    "    W[0] = np.zeros(emb_size, dtype='float32')\n",
    "    # adding a vector for rare words \n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size)\n",
    "    vocab2index[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_count:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1   \n",
    "    return W, np.array(vocab), vocab2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weight, vocab, vocab2index = create_embedding_matrix(word_vecs, word_count, min_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20224"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pretrained_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.1397,  0.0364, -0.0639,  ...,  0.1270,  0.2325,  0.1952],\n",
       "        [-0.3012,  0.3320,  0.1582,  ..., -0.7797,  0.9873,  0.6246],\n",
       "        ...,\n",
       "        [-0.1321, -0.3749, -0.5517,  ...,  0.9329,  0.3362, -0.2008],\n",
       "        [ 0.8547,  0.1916,  0.0963,  ...,  0.1707, -0.2616,  0.5350],\n",
       "        [-0.0111, -1.1496, -0.5092,  ...,  0.0186, -0.9912,  0.1102]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size = 50\n",
    "V = len(pretrained_weight)\n",
    "emb = nn.Embedding(V, emb_size)\n",
    "emb.weight.data.copy_(torch.from_numpy(pretrained_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
